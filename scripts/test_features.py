"""
Test Feature Engineering Pipeline
Validates feature extraction and ML-ready data preparation
"""

import asyncio
import logging
import os
import sys
import psycopg2
import numpy as np
from datetime import datetime, timedelta
from dotenv import load_dotenv

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from src.data.feature_engineering import FeatureEngineer, ProtocolFeatures

logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)s:%(name)s:%(message)s"
)
logger = logging.getLogger(__name__)


def test_feature_engineering():
    """Test feature engineering pipeline"""
    load_dotenv()
    
    db_url = os.getenv("DATABASE_URL")
    if not db_url:
        logger.error("‚ùå DATABASE_URL not configured")
        return
    
    logger.info("\n" + "="*80)
    logger.info("TESTING FEATURE ENGINEERING PIPELINE")
    logger.info("="*80 + "\n")
    
    # Connect to database
    db_conn = psycopg2.connect(db_url)
    
    # Get protocols
    cursor = db_conn.cursor()
    cursor.execute("SELECT id, name FROM protocols ORDER BY id")
    protocols = cursor.fetchall()
    
    if not protocols:
        logger.error("‚ùå No protocols found in database")
        logger.info("Run: python scripts/collect_data.py first")
        return
    
    logger.info(f"‚úÖ Found {len(protocols)} protocols:")
    for pid, name in protocols:
        logger.info(f"   {pid}. {name}")
    
    # Check if we have yield data
    cursor.execute("SELECT COUNT(*) FROM protocol_yields")
    yield_count = cursor.fetchone()[0]
    
    if yield_count == 0:
        logger.warning("‚ö†Ô∏è  No yield data found in database")
        logger.info("Run: python scripts/collect_data.py first")
        cursor.close()
        db_conn.close()
        return
    
    logger.info(f"‚úÖ Found {yield_count} yield records\n")
    cursor.close()
    
    # Initialize feature engineer
    engineer = FeatureEngineer(db_conn)
    
    # Test 1: Create feature vector for a single protocol
    logger.info("üìä Test 1: Single Protocol Feature Extraction")
    logger.info("-" * 80)
    
    protocol_id = protocols[0][0]
    protocol_name = protocols[0][1]
    
    # Get available assets
    cursor = db_conn.cursor()
    cursor.execute("""
        SELECT DISTINCT asset
        FROM protocol_yields
        WHERE protocol_id = %s
        LIMIT 3
    """, (protocol_id,))
    
    assets = [row[0] for row in cursor.fetchall()]
    cursor.close()
    
    if not assets:
        logger.warning(f"‚ö†Ô∏è  No assets found for {protocol_name}")
    else:
        logger.info(f"Testing {protocol_name} with assets: {', '.join(assets)}\n")
        
        for asset in assets[:1]:  # Test with first asset
            try:
                current_date = datetime.now()
                
                features = engineer.create_feature_vector(
                    protocol_id=protocol_id,
                    protocol_name=protocol_name,
                    asset=asset,
                    current_date=current_date
                )
                
                logger.info(f"\n‚úÖ Features created for {protocol_name} - {asset}")
                logger.info(f"   Timestamp: {features.timestamp}")
                logger.info(f"   Current APY: {features.current_apy:.3f}%")
                logger.info(f"   7-day MA APY: {features.apy_7d_ma:.3f}%")
                logger.info(f"   APY Volatility: {features.apy_volatility:.3f}%")
                logger.info(f"   Total Liquidity: ${features.total_liquidity_usd:,.0f}")
                logger.info(f"   Utilization Rate: {features.utilization_rate:.1f}%")
                logger.info(f"   Risk Score: {features.risk_score:.1f}/100")
                logger.info(f"   Gas Price: {features.gas_price_gwei:.2f} gwei")
                
                # Test 2: Convert to numpy vector
                logger.info(f"\nüìä Test 2: Feature Vector Conversion")
                logger.info("-" * 80)
                
                vector = features.to_vector()
                logger.info(f"‚úÖ Feature vector shape: {vector.shape}")
                logger.info(f"   Vector dtype: {vector.dtype}")
                logger.info(f"   First 10 features: {vector[:10]}")
                
                # Check for NaN/Inf
                if np.any(np.isnan(vector)):
                    logger.warning("‚ö†Ô∏è  Vector contains NaN values")
                if np.any(np.isinf(vector)):
                    logger.warning("‚ö†Ô∏è  Vector contains Inf values")
                else:
                    logger.info("‚úÖ Vector is clean (no NaN/Inf)")
                
                # Test 3: Feature names
                logger.info(f"\nüìä Test 3: Feature Names")
                logger.info("-" * 80)
                
                feature_names = ProtocolFeatures.feature_names()
                logger.info(f"‚úÖ Total features: {len(feature_names)}")
                logger.info(f"\n   Feature groups:")
                logger.info(f"   ‚Ä¢ Yield features (4): {feature_names[0:4]}")
                logger.info(f"   ‚Ä¢ Liquidity features (4): {feature_names[4:8]}")
                logger.info(f"   ‚Ä¢ Volume features (3): {feature_names[8:11]}")
                logger.info(f"   ‚Ä¢ Risk features (5): {feature_names[11:16]}")
                logger.info(f"   ‚Ä¢ Market features (4): {feature_names[16:20]}")
                logger.info(f"   ‚Ä¢ Time features (4): {feature_names[20:24]}")
                logger.info(f"   ‚Ä¢ Competitive features (4): {feature_names[24:28]}")
                logger.info(f"   ‚Ä¢ Historical features (4): {feature_names[28:32]}")
                
            except Exception as e:
                logger.error(f"‚ùå Error creating features: {e}")
                import traceback
                traceback.print_exc()
    
    # Test 4: Training dataset creation (if enough data)
    logger.info(f"\nüìä Test 4: Training Dataset Creation")
    logger.info("-" * 80)
    
    # Check data range
    cursor = db_conn.cursor()
    cursor.execute("""
        SELECT MIN(recorded_at), MAX(recorded_at), COUNT(*)
        FROM protocol_yields
    """)
    min_date, max_date, total_records = cursor.fetchone()
    cursor.close()
    
    if min_date and max_date:
        logger.info(f"   Data range: {min_date} to {max_date}")
        logger.info(f"   Total records: {total_records}")
        
        # If we have at least a few records, test dataset creation
        if total_records >= 10:
            try:
                # Use last 3 days of data
                end_date = max_date
                start_date = max_date - timedelta(days=3)
                
                logger.info(f"   Creating dataset for {start_date} to {end_date}")
                
                feature_matrix, feature_objects = engineer.create_training_dataset(
                    start_date=start_date,
                    end_date=end_date,
                    protocols=protocols[:2]  # Test with first 2 protocols
                )
                
                if len(feature_matrix) > 0:
                    logger.info(f"\n‚úÖ Training dataset created")
                    logger.info(f"   Shape: {feature_matrix.shape}")
                    logger.info(f"   Samples: {feature_matrix.shape[0]}")
                    logger.info(f"   Features per sample: {feature_matrix.shape[1]}")
                    logger.info(f"   Feature objects: {len(feature_objects)}")
                    
                    # Statistics
                    logger.info(f"\n   Dataset statistics:")
                    logger.info(f"   ‚Ä¢ Mean values: {np.mean(feature_matrix, axis=0)[:5]}")
                    logger.info(f"   ‚Ä¢ Std dev: {np.std(feature_matrix, axis=0)[:5]}")
                    logger.info(f"   ‚Ä¢ Min values: {np.min(feature_matrix, axis=0)[:5]}")
                    logger.info(f"   ‚Ä¢ Max values: {np.max(feature_matrix, axis=0)[:5]}")
                else:
                    logger.warning("‚ö†Ô∏è  No features generated for dataset")
                    
            except Exception as e:
                logger.error(f"‚ùå Error creating training dataset: {e}")
                import traceback
                traceback.print_exc()
        else:
            logger.info("   ‚ÑπÔ∏è  Not enough data for training dataset (need 10+ records)")
            logger.info("   Collect more data first: python scripts/collect_data.py")
    else:
        logger.warning("‚ö†Ô∏è  No date range found in protocol_yields")
    
    db_conn.close()
    
    # Summary
    logger.info("\n" + "="*80)
    logger.info("FEATURE ENGINEERING TEST SUMMARY")
    logger.info("="*80)
    logger.info("‚úÖ Feature extraction: Working")
    logger.info("‚úÖ Vector conversion: Working")
    logger.info("‚úÖ 32-dimensional feature space: Implemented")
    logger.info("\nüìù Feature groups:")
    logger.info("   ‚Ä¢ Yield (4): APY, moving averages, volatility")
    logger.info("   ‚Ä¢ Liquidity (4): TVL, utilization, trends")
    logger.info("   ‚Ä¢ Volume (3): 24h/7d volumes, trends")
    logger.info("   ‚Ä¢ Risk (5): Risk score, exploits, audits")
    logger.info("   ‚Ä¢ Market (4): Gas, ETH price, volatility, dominance")
    logger.info("   ‚Ä¢ Time (4): Day, hour, weekend, epoch")
    logger.info("   ‚Ä¢ Competitive (4): Rank, vs averages")
    logger.info("   ‚Ä¢ Historical (4): ROI, Sharpe, drawdown")
    logger.info("\n‚úÖ Ready for ML model training!")
    logger.info("\nüìù Next steps:")
    logger.info("   1. Collect more historical data")
    logger.info("   2. Train LSTM yield predictor (Week 5-6)")
    logger.info("   3. Train XGBoost risk classifier (Week 7-8)")


if __name__ == "__main__":
    test_feature_engineering()
